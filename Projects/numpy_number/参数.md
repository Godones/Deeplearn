激活层使用sigmoid时，learning_rate = 0.01, epoch = 20000,优化器使用Momentum

### 权重的初始化

sigmoid : 使用Xavier初始值,即使用标准差为$\frac{1}{\sqrt{n}}$,n为前一层的节点数

Relu : 使用He初始值，即使用标准差为$\sqrt{\frac{2}{n}}$

不可将权重初始值设置为0，此时会导致神经网络拥有相同的权重，那么其就会丧失对不同参数的具有不同权重的优点，为了防止“权重均一化”，（瓦解权重的对称结构），必须随机生成初始值。

###### 隐藏层激活值的分布：

权重初始值对隐藏层激活值的分布具有影响。

不同标准差的高斯分布初始值会带来**梯度消失**（激活值分布在0，1附近）与**表现力受限（激活值过于集中）**。

###### Batch normalizetion ：

1. 可以增大学习率
2. 不过分依赖初始值
3. 抑制过拟合现象





