{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    '''平方差函数'''\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    '''当维度为1的时候'''\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val # 还原值\n",
    "\n",
    "    return grad\n",
    "def numerical_gradient(f, X):\n",
    "    '''求梯度的函数'''\n",
    "    if X.ndim == 1:\n",
    "        '''如果x的维度是1'''\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(f, init_x, lr = 0.01, step_num=100):#梯度下降\n",
    "    '''进行梯度下降的函数'''\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''sigmoid函数'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    '''softmax函数，用于分类问题'''\n",
    "    c = np.max(x)\n",
    "    exp_a = np.exp(x-c) ##防止数据溢出\n",
    "    sum_exp = np.sum(exp_a)\n",
    "    return exp_a/sum_exp\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    '''交叉熵误差函数'''\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([6., 8.])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function, np.array([3.0, 4.0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([-6.11110793e-10,  8.14814391e-10]), 1.0373788922158197e-18)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "result = gradient_descent(function, init_x=init_x, lr=0.1, step_num=100)\n",
    "result,function(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.w = np.random.randn(2,3)\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.w)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35045158 -0.48679263  0.22397527]\n",
      " [-0.49633575 -0.11827549 -1.05972292]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.23643122, -0.39852352, -0.81936547])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "1.4619880397509868"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.argmax()\n",
    "t = np.array([0,0,1])\n",
    "net.loss(x, t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.24910513,  0.21182971, -0.46093484],\n       [ 0.37365769,  0.31774457, -0.69140226]])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda w : net.loss(x, t)\n",
    "dw = numerical_gradient(f, net.w)\n",
    "dw\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}